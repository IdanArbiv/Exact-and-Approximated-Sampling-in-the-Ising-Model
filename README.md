# Exact and Approximated Sampling in the Ising Model

# Exact Sampling in the Ising Model via Dynamic Programming
This repository provides implementations of exact and approximated sampling methods for the Ising model. The Ising model is a mathematical model used to describe the behavior of magnetic spins in a lattice. In this project, we specifically focus on a 2D regular lattice with dimensions 8 × 8.

# Exact Sampling via Dynamic Programming
The file contains an implementation of an exact sampler for the Ising model using dynamic programming. The sampler generates random samples at three different temperatures and displays ten samples for each temperature. Additionally, it computes two empirical expectations based on the generated samples.

# Approximated Sampling via MCMC (Markov Chain Monte Carlo)
The file contains an implementation of the Gibbs sampler, an approximated sampling method for the Ising model using the MCMC approach. The Gibbs sampler is used to compute expectations and compare them with the exact results obtained from the dynamic programming approach. Furthermore, the code demonstrates image restoration from Ising-model samples corrupted by noise.

# Dependencies
The code is implemented in Python and requires the following dependencies:

- numpy
- matplotlib

# Results and Analysis

# Exact Sampling in the Ising Model via Dynamic Programming
Here are ten independent samples (each such sample is an entire 8 × 8 image) at each of the three temperatures.
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/daf0d06e-bb55-4bfc-9518-1f0ef090ef6b)
In the context of the Ising model, the temperature parameter assumes the role of regulating the level of consensus or dissension among adjacent vertices within the lattice. From the lecture, we observed that as the temperature approaches infinity, convergence towards a constant state occurs, leading to the establishment of a uniform distribution. Conversely, as the temperature approaches zero, the lattice tends toward a monochromatic configuration.

Our findings align with our initial expectations. Examining the results reveals that a temperature value of 1 yields outcomes that closely resemble a monochromatic scheme. Predominantly, the images are either entirely black or entirely white. By increasing the temperature to 1.5 and 2, we permit greater variation among neighboring vertices in the lattice. Consequently, the results become less smooth, moving closer to a uniform distribution.

# Approximated Sampling in the Ising model via MCMC
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/23241954-3fd7-4564-ae2e-017e9a2348a1)

During our educational session, we were instructed on the attainment of a uniform distribution when the temperature tends to infinity. Furthermore, we were advised to expect analogous values for neighboring grid locations that have short spatial distances.

Our research findings, as well as Exercise 7, validate that when the  Temp = 1, an "all-black" or "all-white" image is commonly generated. Consequently, the values of X_((1,1)) are consistently similar to X_((2,2))) and X_((8,8))  in nearly all instances.

When Temp = 1.5, the values of X_((1,1)) resemble those of X_((2,2))
to a greater extent than X_((8,8)), primarily due to the increased spatial distance between X_((1,1)) and X_((8,8)). This greater distance diminishes the likelihood of them sharing the same value.

At Temp = 2, noteworthy disparities arise between X_((1,1)) and X_((2,2)), supporting the proposition that a uniform distribution is attained as the temperature approaches infinity, thereby severing any relationship between neighboring elements in the lattice. X_((1,1)) and X_((8,8)) exhibit dissimilarities and display varying values in nearly all scenarios.

# Independent samples:
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/0407ddde-5405-415c-8d5e-c8d109566dd6)

# Ergodicity:
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/8495e4f4-898c-4ab5-b2dd-95c1b97bee81)

When comparing the two methods employed in this analysis to the exact sampling conducted in problem 1, several observations emerge:

1. The outcomes generated by the ergodicity method exhibit a strong resemblance to the results obtained through exact sampling, which aligns with fact  as presented in the Gibbs Sampling slides – 
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/d6bec30c-be53-4135-b515-c8a5a1d1c261)

This fact implies that as we increase the number of iterations towards infinity, we approach the true distribution. Given the substantial number of 25,000 sweeps performed in this exercise, it is reasonable to anticipate that the findings closely approximate the actual distribution.

2. In all methods employed, X_((1,1) ) and X_((2,2) ) display much greater similarity compared to X_((1,1) ) and X_((8,8) ). This observation can be attributed to the smaller spatial distance between the former pair of sites.

3. Notably, the outcome for E ̂_Temp (X_((1,1) ),X_((2,2) )) obtained from the independent samples method exhibits a significant deviation from the results obtained from the other methods. This discrepancy can be attributed to the combination of a large spatial distance and a limited number of sweeps, which can cause substantial divergence from the actual distribution, even when the expectation is computed over 10,000 samples. As observed in the ergodicity method, achieving proximity to the true distribution for a single sample necessitates a considerable number of sweeps. Therefore, it can be inferred that 25 sweeps may not be sufficient for the expectation over 10,000 samples to closely approximate the actual distribution.

![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/0b7d9717-631a-49ef-8eb0-f069a5676439)
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/49f55e50-9ebd-46f2-98d4-9d75ed751dba)
![image](https://github.com/IdanArbiv/Computer-Vision-Sampling-Methods/assets/101040591/fa96fb02-cca5-478d-8572-bf104e7c732c)

The primary objective of this experiment was to generate a lattice and introduce Gaussian noise to investigate different methods for reproducing the original lattice. As part of our approach, we employed the Ising model as a prior, examining its impact on the lattice when varying the temperature parameter. Notably, decreasing the temperature yielded smoother results, as the Ising model became more restrictive in allowing disagreements among neighboring vertices within the lattice.

We will now delve into a detailed analysis of each method employed to restore the original lattice:

# Gibbs Sampling with Ising Model Prior:
This technique leverages the Ising model prior, effectively integrating prior knowledge pertaining to the structure of the lattice. By utilizing Gibbs Sampling, an iterative algorithm, each pixel's value is updated based on its neighboring pixels and the noise present in the image. By considering the spatial relationships between pixels and incorporating the Ising model prior, this method aims to reconstruct the underlying lattice structure while accounting for Gaussian noise. The resulting reconstruction is influenced by the interactions among neighboring pixels and the prior assumptions embedded within the Ising model.

# ICM Method with Posterior Distribution:
The ICM method, takes a distinct approach by directly optimizing the posterior distribution. Through iterative steps, this method assigns values to each pixel that maximize the posterior probability, considering both the observed noisy image and the Ising model prior. The ICM method primarily focuses on identifying the most plausible lattice configuration that aligns with the observed data and the prior information. By maximizing the posterior distribution, this technique aims to produce a reconstruction that best fits the observed image while adhering to the inherent lattice structure.

# MLE (Maximum Likelihood Estimation):
MLE, or Maximum Likelihood Estimation, constitutes a statistical method used to estimate model parameters by maximizing the likelihood of the observed data. In the context of lattice reconstruction with Gaussian noise, MLE aims to identify the lattice configuration that maximizes the likelihood of observing the given noisy image. Notably, MLE does not explicitly incorporate prior knowledge or consider the underlying lattice structure. Instead, it focuses solely on finding the most probable lattice configuration based solely on the observed data.






